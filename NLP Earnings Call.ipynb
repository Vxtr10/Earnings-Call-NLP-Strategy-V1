{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f9fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install yfinance\n",
    "%pip install pysentiment2\n",
    "%pip install transformers\n",
    "%pip install py-readability-metrics\n",
    "%python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f21424b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pysentiment2 as ps\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import datetime as dt\n",
    "import spacy\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from readability import Readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e93fc6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MMM', 'AOS', 'ABT', 'ABBV', 'ABMD', 'ACN', 'ATVI', 'ADM', 'ADBE', 'ADP', 'AAP', 'AES', 'AFL', 'A', 'APD', 'AKAM', 'ALK', 'ALB', 'ARE', 'ALGN', 'ALLE', 'LNT', 'ALL', 'GOOGL', 'GOOG', 'MO', 'AMZN', 'AMCR', 'AMD', 'AEE', 'AAL', 'AEP', 'AXP', 'AIG', 'AMT', 'AWK', 'AMP', 'ABC', 'AME', 'AMGN', 'APH', 'ADI', 'ANSS', 'AON', 'APA', 'AAPL', 'AMAT', 'APTV', 'ANET', 'AJG', 'AIZ', 'T', 'ATO', 'ADSK', 'AZO', 'AVB', 'AVY', 'BKR', 'BALL', 'BAC', 'BBWI', 'BAX', 'BDX', 'WRB', 'BRK.B', 'BBY', 'BIO', 'TECH', 'BIIB', 'BLK', 'BK', 'BA', 'BKNG', 'BWA', 'BXP', 'BSX', 'BMY', 'AVGO', 'BR', 'BRO', 'BF.B', 'CHRW', 'CDNS', 'CZR', 'CPT', 'CPB', 'COF', 'CAH', 'KMX', 'CCL', 'CARR', 'CTLT', 'CAT', 'CBOE', 'CBRE', 'CDW', 'CE', 'CNC', 'CNP', 'CDAY', 'CF', 'CRL', 'SCHW', 'CHTR', 'CVX', 'CMG', 'CB', 'CHD', 'CI', 'CINF', 'CTAS', 'CSCO', 'C', 'CFG', 'CTXS', 'CLX', 'CME', 'CMS', 'KO', 'CTSH', 'CL', 'CMCSA', 'CMA', 'CAG', 'COP', 'ED', 'STZ', 'CEG', 'COO', 'CPRT', 'GLW', 'CTVA', 'COST', 'CTRA', 'CCI', 'CSX', 'CMI', 'CVS', 'DHI', 'DHR', 'DRI', 'DVA', 'DE', 'DAL', 'XRAY', 'DVN', 'DXCM', 'FANG', 'DLR', 'DFS', 'DISH', 'DIS', 'DG', 'DLTR', 'D', 'DPZ', 'DOV', 'DOW', 'DTE', 'DUK', 'DRE', 'DD', 'DXC', 'EMN', 'ETN', 'EBAY', 'ECL', 'EIX', 'EW', 'EA', 'ELV', 'LLY', 'EMR', 'ENPH', 'ETR', 'EOG', 'EPAM', 'EFX', 'EQIX', 'EQR', 'ESS', 'EL', 'ETSY', 'RE', 'EVRG', 'ES', 'EXC', 'EXPE', 'EXPD', 'EXR', 'XOM', 'FFIV', 'FDS', 'FAST', 'FRT', 'FDX', 'FITB', 'FRC', 'FE', 'FIS', 'FISV', 'FLT', 'FMC', 'F', 'FTNT', 'FTV', 'FBHS', 'FOXA', 'FOX', 'BEN', 'FCX', 'GRMN', 'IT', 'GNRC', 'GD', 'GE', 'GIS', 'GM', 'GPC', 'GILD', 'GL', 'GPN', 'GS', 'HAL', 'HIG', 'HAS', 'HCA', 'PEAK', 'HSIC', 'HSY', 'HES', 'HPE', 'HLT', 'HOLX', 'HD', 'HON', 'HRL', 'HST', 'HWM', 'HPQ', 'HUM', 'HBAN', 'HII', 'IBM', 'IEX', 'IDXX', 'ITW', 'ILMN', 'INCY', 'IR', 'INTC', 'ICE', 'IP', 'IPG', 'IFF', 'INTU', 'ISRG', 'IVZ', 'IQV', 'IRM', 'JBHT', 'JKHY', 'J', 'JNJ', 'JCI', 'JPM', 'JNPR', 'K', 'KDP', 'KEY', 'KEYS', 'KMB', 'KIM', 'KMI', 'KLAC', 'KHC', 'KR', 'LHX', 'LH', 'LRCX', 'LW', 'LVS', 'LDOS', 'LEN', 'LNC', 'LIN', 'LYV', 'LKQ', 'LMT', 'L', 'LOW', 'LUMN', 'LYB', 'MTB', 'MRO', 'MPC', 'MKTX', 'MAR', 'MMC', 'MLM', 'MAS', 'MA', 'MTCH', 'MKC', 'MCD', 'MCK', 'MDT', 'MRK', 'META', 'MET', 'MTD', 'MGM', 'MCHP', 'MU', 'MSFT', 'MAA', 'MRNA', 'MHK', 'MOH', 'TAP', 'MDLZ', 'MPWR', 'MNST', 'MCO', 'MS', 'MOS', 'MSI', 'MSCI', 'NDAQ', 'NTAP', 'NFLX', 'NWL', 'NEM', 'NWSA', 'NWS', 'NEE', 'NLSN', 'NKE', 'NI', 'NDSN', 'NSC', 'NTRS', 'NOC', 'NLOK', 'NCLH', 'NRG', 'NUE', 'NVDA', 'NVR', 'NXPI', 'ORLY', 'OXY', 'ODFL', 'OMC', 'ON', 'OKE', 'ORCL', 'OGN', 'OTIS', 'PCAR', 'PKG', 'PARA', 'PH', 'PAYX', 'PAYC', 'PYPL', 'PENN', 'PNR', 'PEP', 'PKI', 'PFE', 'PM', 'PSX', 'PNW', 'PXD', 'PNC', 'POOL', 'PPG', 'PPL', 'PFG', 'PG', 'PGR', 'PLD', 'PRU', 'PEG', 'PTC', 'PSA', 'PHM', 'PVH', 'QRVO', 'PWR', 'QCOM', 'DGX', 'RL', 'RJF', 'RTX', 'O', 'REG', 'REGN', 'RF', 'RSG', 'RMD', 'RHI', 'ROK', 'ROL', 'ROP', 'ROST', 'RCL', 'SPGI', 'CRM', 'SBAC', 'SLB', 'STX', 'SEE', 'SRE', 'NOW', 'SHW', 'SBNY', 'SPG', 'SWKS', 'SJM', 'SNA', 'SEDG', 'SO', 'LUV', 'SWK', 'SBUX', 'STT', 'STE', 'SYK', 'SIVB', 'SYF', 'SNPS', 'SYY', 'TMUS', 'TROW', 'TTWO', 'TPR', 'TGT', 'TEL', 'TDY', 'TFX', 'TER', 'TSLA', 'TXN', 'TXT', 'TMO', 'TJX', 'TSCO', 'TT', 'TDG', 'TRV', 'TRMB', 'TFC', 'TWTR', 'TYL', 'TSN', 'USB', 'UDR', 'ULTA', 'UNP', 'UAL', 'UPS', 'URI', 'UNH', 'UHS', 'VLO', 'VTR', 'VRSN', 'VRSK', 'VZ', 'VRTX', 'VFC', 'VTRS', 'VICI', 'V', 'VNO', 'VMC', 'WAB', 'WBA', 'WMT', 'WBD', 'WM', 'WAT', 'WEC', 'WFC', 'WELL', 'WST', 'WDC', 'WRK', 'WY', 'WHR', 'WMB', 'WTW', 'GWW', 'WYNN', 'XEL', 'XYL', 'YUM', 'ZBRA', 'ZBH', 'ZION', 'ZTS']\n"
     ]
    }
   ],
   "source": [
    "spy_list_requests = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "soup = BeautifulSoup(spy_list_requests.text, 'lxml')\n",
    "table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "tickers = []\n",
    "for row in table.findAll('tr')[1:]:\n",
    "    ticker = row.findAll('td')[0].text\n",
    "    tickers.append(ticker)\n",
    "\n",
    "spy_tickers_list = [s.replace('\\n', '') for s in tickers]\n",
    "\n",
    "print(spy_tickers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f971934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_transcript(ticker, date_yr, date_qtr, i):\n",
    "    try:\n",
    "        url = 'https://roic.ai/transcripts/' + ticker + '?y=' + str(date_yr) + '&q=' + str(date_qtr)\n",
    "        html_text = requests.get(url).text\n",
    "        soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "        # https://jsonformatter.org/\n",
    "        # finds the 15th <script/> and strips the unnecsssary data so it can be read in JSON\n",
    "        script = soup.find_all('script')[15].text.strip()  \n",
    "        data = json.loads(script)\n",
    "        transcript_data = data['props']['pageProps']['transcriptdata']['content']  # loads the transcript content\n",
    "        date1 = datetime.strptime(data['props']['pageProps']['data']['data']['earningscalls'][i]['date'], '%Y-%m-%d %H:%M:%S')\n",
    "        #date1 = 'day 0', earnings transcript release date\n",
    "        ntr = yf.Ticker(ticker)\n",
    "        hist1 = ntr.history(start=date1, end=date1 + timedelta(days=10))  # day 0 to day 10\n",
    "        hist2 = ntr.history(start=date1, end=date1 + timedelta(days=30))   # day 0 to day 30\n",
    "        hist3 = ntr.history(start=date1, end=date1 + timedelta(days=50))\n",
    "        hist4 = ntr.history(start=date1, end=date1 + timedelta(days=70))      \n",
    "        hist5 = ntr.history(start=date1, end=date1 + timedelta(days=90))   \n",
    "        hist6 = ntr.history(start=date1 + timedelta(days=1), end=date1 + timedelta(days=10))   #day 1 to day 11\n",
    "        hist7 = ntr.history(start=date1 + timedelta(days=1), end=date1 + timedelta(days=30))   #day 1 to day 31\n",
    "        hist8 = ntr.history(start=date1 + timedelta(days=1), end=date1 + timedelta(days=50))   \n",
    "        hist9 = ntr.history(start=date1 + timedelta(days=1), end=date1 + timedelta(days=70))\n",
    "        hist10 = ntr.history(start=date1 + timedelta(days=1), end=date1 + timedelta(days=90))   \n",
    "        percentage_change1 = find_percentage_change(hist1)\n",
    "        percentage_change2 = find_percentage_change(hist2)\n",
    "        percentage_change3 = find_percentage_change(hist3)\n",
    "        percentage_change4 = find_percentage_change(hist4)\n",
    "        percentage_change5 = find_percentage_change(hist5)\n",
    "        percentage_change6 = find_percentage_change(hist6)\n",
    "        percentage_change7 = find_percentage_change(hist7)\n",
    "        percentage_change8 = find_percentage_change(hist8)\n",
    "        percentage_change9 = find_percentage_change(hist9)\n",
    "        percentage_change10 = find_percentage_change(hist10)\n",
    "            \n",
    "        return transcript_data, percentage_change1, percentage_change2, percentage_change3, percentage_change4, percentage_change5, percentage_change6, percentage_change7, percentage_change8, percentage_change9, percentage_change10\n",
    "\n",
    "    except:\n",
    "        return None, None, None, None, None, None, None, None, None, None, None \n",
    "\n",
    "def find_percentage_change(hist):\n",
    "    try:\n",
    "        percentage_change = ((hist['Open'][-1])/(hist['Open'][0])-1) # day 1 to day +30\n",
    "    except:\n",
    "        return None\n",
    "        \n",
    "    return percentage_change\n",
    "\n",
    "def ticker_transcript(ticker):\n",
    "    url = 'https://roic.ai/transcripts/' + ticker\n",
    "\n",
    "    html_text = requests.get(url).text\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "\n",
    "    script = soup.find_all('script')[15].text.strip()\n",
    "    data = json.loads(script)\n",
    "    first_er_date = data['props']['pageProps']['data']['data']['earningscalls'][0]\n",
    "    last_er_date = data['props']['pageProps']['data']['data']['earningscalls'][-1]\n",
    "\n",
    "    first_er_date_yr = first_er_date['year']\n",
    "    first_er_date_qtr = first_er_date['quarter']\n",
    "    last_er_date_yr = last_er_date['year']\n",
    "\n",
    "    yr_difference = first_er_date_yr-last_er_date_yr\n",
    "    date_yr = first_er_date_yr\n",
    "    date_qtr = first_er_date_qtr\n",
    "\n",
    "    arr_ticker_infos = np.empty((0, 13), str)\n",
    "    arr_full_transcript = np.array([], str) #array of all of the transcripts, every year and quarter\n",
    "    q = -1\n",
    "    for i in range(0, yr_difference + 1):\n",
    "        if date_yr == first_er_date_yr:\n",
    "            for j in range(0, first_er_date_qtr):\n",
    "                q += 1\n",
    "                full_transcript, percentage_change1, percentage_change2, percentage_change3, percentage_change4, percentage_change5, percentage_change6, percentage_change7, percentage_change8, percentage_change9, percentage_change10 = find_transcript(ticker, date_yr, date_qtr, q)\n",
    "                arr_full_transcript = np.append(arr_full_transcript, full_transcript)\n",
    "                arr_ticker_infos = np.append(arr_ticker_infos, np.array([[ticker, date_yr, date_qtr, percentage_change1, percentage_change2, percentage_change3, percentage_change4, percentage_change5, percentage_change6, percentage_change7, percentage_change8, percentage_change9, percentage_change10]]), axis=0)\n",
    "                date_qtr -= 1\n",
    "                if date_qtr == 0:\n",
    "                    date_qtr = 4\n",
    "        else:\n",
    "            for j in range(0, 4):\n",
    "                q += 1\n",
    "                full_transcript, percentage_change1, percentage_change2, percentage_change3, percentage_change4, percentage_change5, percentage_change6, percentage_change7, percentage_change8, percentage_change9, percentage_change10 = find_transcript(ticker, date_yr, date_qtr, q)\n",
    "                arr_full_transcript = np.append(arr_full_transcript, full_transcript)\n",
    "                arr_ticker_infos = np.append(arr_ticker_infos, np.array([[ticker, date_yr, date_qtr, percentage_change1, percentage_change2, percentage_change3, percentage_change4, percentage_change5, percentage_change6, percentage_change7, percentage_change8, percentage_change9, percentage_change10]]), axis=0)\n",
    "\n",
    "                date_qtr -= 1\n",
    "                if date_qtr == 0:\n",
    "                    date_qtr = 4\n",
    "        date_yr -= 1\n",
    "    #1. gets tf idf, and cosine similarity\n",
    "    arr_tf_idf = np.array([])\n",
    "    cleaned_transcript_list = np.array([])\n",
    "    flipped_full_transcript = arr_full_transcript[::-1] #flip order\n",
    "    \n",
    "    for i in range(0, len(flipped_full_transcript)):\n",
    "        cleaned_transcript_list = np.array([])\n",
    "        try:\n",
    "            rolling_transcript = flipped_full_transcript[i - 4:i]\n",
    "            for j in rolling_transcript:\n",
    "                single_transcript = clean_text(j.split(\"\\n\"))\n",
    "                \n",
    "                cleaned_transcript_list = np.append(cleaned_transcript_list, np.array([single_transcript]))\n",
    "            tfidf_vec = tf_idf(''.join(cleaned_transcript_list))\n",
    "            arr_tf_idf = np.append(arr_tf_idf, np.array([tfidf_vec]))\n",
    "            \n",
    "        except:\n",
    "            tfidf_vec = None\n",
    "            arr_tf_idf = np.append(arr_tf_idf, np.array([tfidf_vec]))\n",
    "            continue\n",
    "\n",
    "    arr_tf_idf = arr_tf_idf[::-1] #flip order\n",
    " \n",
    "    arr_cosine_similarities = find_cosine_similarities(arr_tf_idf)\n",
    "    arr_cosine_similarities = np.reshape(arr_cosine_similarities, (arr_cosine_similarities.shape[0], 1))\n",
    "    arr_ticker_infos = np.concatenate((arr_ticker_infos, arr_cosine_similarities), axis=1)\n",
    "    \n",
    "    #2. gets Loughran and Mcdonalds Sentiment Score [positiveA, negativeA, polarityA, subjectivityA, positiveB, negativeB, polarityB, subjectivityB]\n",
    "    arr_full_LM = find_LM(arr_full_transcript) #returns a (#, 8) 2D list\n",
    "    arr_ticker_infos = np.concatenate((arr_ticker_infos, arr_full_LM), axis=1) \n",
    "    \n",
    "    #3. gets word complexity information (including Flesch Kincaid, Flesch, Gunning Fog, and Smog index)\n",
    "    arr_word_complexity = get_arr_word_complexity(arr_full_transcript)\n",
    "    arr_ticker_infos = np.concatenate((arr_ticker_infos, arr_word_complexity), axis=1)\n",
    "    \n",
    "    # Include Finbert?\n",
    "    \n",
    "    #Deletes the most recent data (first index) since 90 days has not passed yet.\n",
    "    arr_ticker_infos = arr_ticker_infos[1:,:]\n",
    "    #arr_ticker_infos in the 2D array form of [['AAPL' '2022' '2'...], ['AAPL' '2022' '1'...], ['AAPL' '2021' '4'...]]\n",
    "    #['ticker', 'yr', 'qtr', '%change etc', 'tf-idf_cos_sim', LM{'posA' 'negA' 'polA' 'subA' 'posB' 'negB' 'polB' 'subB'}, Complexity{FKsafe harbour' 'FKQ&A' 'GFsafe harbour' 'GFQ&A' ...}] \n",
    "    return arr_ticker_infos \n",
    "\n",
    "    \n",
    "def get_arr_word_complexity(arr_full_transcript):\n",
    "    arr_word_complexity = np.array([])\n",
    "    for transcript in arr_full_transcript:\n",
    "        try:\n",
    "            cleaned_safe_harbour, cleaned_questions = split_transcript(transcript)\n",
    "\n",
    "            fk_score_safe_harbour = find_flesch_kincaid(cleaned_safe_harbour)\n",
    "            fk_score_questions = find_flesch_kincaid(cleaned_questions)\n",
    "\n",
    "            gf_score_safe_harbour = find_gunning_fog(cleaned_safe_harbour)\n",
    "            gf_score_questions = find_gunning_fog(cleaned_questions)\n",
    "\n",
    "            smog_score_safe_harbour = find_smog(cleaned_safe_harbour)\n",
    "            smog_score_questions = find_smog(cleaned_questions)\n",
    "\n",
    "            fe_score_safe_harbour = find_flesch(cleaned_safe_harbour)\n",
    "            fe_score_questions = find_flesch(cleaned_questions)\n",
    "            \n",
    "        except:\n",
    "            fk_score_safe_harbour = None  \n",
    "            fk_score_questions = None\n",
    "            gf_score_safe_harbour = None\n",
    "            gf_score_questions = None\n",
    "            smog_score_safe_harbour = None\n",
    "            smog_score_questions = None\n",
    "            fe_score_safe_harbour = None\n",
    "            fe_score_questions = None\n",
    "        arr_word_complexity = np.append(arr_word_complexity, np.array([fk_score_safe_harbour, fk_score_questions, gf_score_safe_harbour, gf_score_questions, smog_score_safe_harbour, smog_score_questions, fe_score_safe_harbour, fe_score_questions]))\n",
    "    arr_word_complexity = arr_word_complexity.reshape(arr_full_transcript.shape[0],8)#change 2 to total outputs \n",
    "    return arr_word_complexity\n",
    "\n",
    "\n",
    "def find_flesch(text):\n",
    "    try:\n",
    "        r = Readability(text)\n",
    "        f = r.flesch()\n",
    "        return f.score\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def find_smog(text):\n",
    "    try:\n",
    "        r = Readability(text)\n",
    "        smog = r.smog()\n",
    "        return smog.score\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def find_gunning_fog(text):\n",
    "    try:\n",
    "        r = Readability(text)\n",
    "        gf= r.gunning_fog()\n",
    "        return gf.score\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def find_flesch_kincaid(text):\n",
    "    try:\n",
    "        r = Readability(text)\n",
    "        fk = r.flesch_kincaid()\n",
    "        return fk.score\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def split_transcript(mystr):    \n",
    "    mystr = mystr.lower()\n",
    "    mystr = mystr.split(\"\\n\")\n",
    "    transcript_safe_harbour, transcript_questions = \"\", \"\"\n",
    "    p = 0\n",
    "    for i in range(0, len(mystr)):\n",
    "        if (\"first question\" in mystr[i] and \"operator\" in mystr[i]) or (i>1 and p==0 and (\"first question\" in mystr[i] or \"go ahead\" in mystr[i]) or (i>2 and \"first\" in mystr[i] and \"operator instructions\" in mystr[i])):\n",
    "            p = 1\n",
    "            transcript_safe_harbour = mystr[0:i+1]\n",
    "            transcript_questions = mystr[i + 1:-1]\n",
    "    cleaned_safe_harbour = clean_text(transcript_safe_harbour)\n",
    "    cleaned_questions = clean_text(transcript_questions)\n",
    "\n",
    "    return cleaned_safe_harbour, cleaned_questions\n",
    "\n",
    "def find_LM(arr_full_transcript): #arr_full_transcript is 1D\n",
    "    arr_full_LM = np.array([])\n",
    "    cleaned_safe_harbour, cleaned_questions = \"\", \"\"\n",
    "    temporary_val = 0\n",
    "    for transcript in arr_full_transcript:\n",
    "        temporary_val += 1\n",
    "        try:\n",
    "            cleaned_safe_harbour, cleaned_questions = split_transcript(transcript)\n",
    "            LM_sentiment_safe_harbour = np.array(find_LM_score(cleaned_safe_harbour))\n",
    "            LM_sentiment_questions = np.array(find_LM_score(cleaned_questions))\n",
    "            arr_full_LM = np.append(arr_full_LM, LM_sentiment_safe_harbour)\n",
    "            arr_full_LM = np.append(arr_full_LM, LM_sentiment_questions)\n",
    "        except:\n",
    "            arr_full_LM = np.append(arr_full_LM, np.array([None, None, None, None, None, None, None, None]))\n",
    "\n",
    "    arr_full_LM = np.reshape(arr_full_LM, (int((arr_full_LM.shape[0])/8), 8))\n",
    "    return arr_full_LM #returns a (#even, 8) 2D list\n",
    "\n",
    "def find_LM_score(text): #text is a string\n",
    "    lm = ps.LM()\n",
    "    tokens = lm.tokenize(text)\n",
    "    score = lm.get_score(tokens) #score is a dictionary\n",
    "    LM_score = list(score.values()) #turns into a 1D list\n",
    "    return LM_score #returns a 1D list\n",
    "\n",
    "def tf_idf(transcript):\n",
    "    #1. Removes stop words, 2. finds tf.idf value, used as a weight\n",
    "    transcript = remove_stop_words(transcript)\n",
    "    transcript = list(transcript.splitlines())\n",
    "    vectoriser = TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        max_features=100,\n",
    "        ngram_range=(1, 3), # 1 to trigram as they are all common in finance (i.e. earnings per share, free cash flow etc.)\n",
    "        stop_words='english'\n",
    "    )\n",
    "    tfidf_vec = vectoriser.fit_transform(transcript)\n",
    "    # tfid_tokens = vectoriser.get_feature_names_out()\n",
    "    # rowlist = []\n",
    "    # for i in range(0, len(transcript)):\n",
    "    #     rowlist.append('sentence' + str(i))\n",
    "    # df_tfidvec = pd.DataFrame(data=sent_vt.toarray(), columns=tfid_tokens)\n",
    "    # df_tfidvec = df_tfidvec.T\n",
    "    # df_tfidvec = df_tfidvec.iloc[:, :].mean(axis=1)\n",
    "    # \n",
    "    # df_tfidvec = pd.DataFrame(data=df_tfidvec, columns=['sentence'])\n",
    "    # df_tfidvec = df_tfidvec.sort_values(by=['sentence'], ascending=False)\n",
    "    return tfidf_vec\n",
    "\n",
    "def find_cosine_similarities(arr_tf_idf):\n",
    "    arr_cosine_similarities = np.array([])\n",
    "    for i in range(0, (arr_tf_idf.shape[0])):\n",
    "        try:\n",
    "            cosine_similarities = linear_kernel(arr_tf_idf[i], arr_tf_idf[i+1])\n",
    "        except:\n",
    "            cosine_similarities = None\n",
    "        arr_cosine_similarities = np.append(arr_cosine_similarities, cosine_similarities)\n",
    "    return arr_cosine_similarities\n",
    " \n",
    "def clean_text(transcript): #transcript is in format [\"a\", \"b\", \"c\"]\n",
    "    transcript = '\\n'.join(transcript)\n",
    "    transcript = transcript.lower()\n",
    "    # turns 'end sentence.start' to 'end sentence. start' with space in between\n",
    "    transcript = re.sub(r'\\.([a-zA-Z])', r'. \\1', transcript)\n",
    "    transcript = re.sub(r'\\?([a-zA-Z])', r'. \\1', transcript)\n",
    "    transcript = re.sub(r'\\!([a-zA-Z])', r'. \\1', transcript)\n",
    "    # replace q1,2,3,4 with q\n",
    "    transcript = re.sub(\"q[1-4]\", \"q\", transcript)\n",
    "    # replace 20xx with 2000\n",
    "    transcript = re.sub(\"20[0-2][0-9]\", \"2000\", transcript)\n",
    "    # deletes all commments that begins with 'Operator: ...'\n",
    "    temp = transcript.split('\\n') #TURNS BACK TO LIST\n",
    "    i = 0\n",
    "    r = 0\n",
    "    try:\n",
    "        while (i != len(temp) - 1) and r < 80:\n",
    "            r+=1\n",
    "            if 'operator:' in temp[i]:\n",
    "                del temp[i]\n",
    "            i += 1\n",
    "            \n",
    "    except:\n",
    "        temp = temp\n",
    "    temp = '\\n'.join(temp)\n",
    "    temp = re.sub(r'\\.([a-zA-Z])', r'. \\1', temp)\n",
    "    temp = re.sub(r'\\?([a-zA-Z])', r'. \\1', temp)\n",
    "    temp = re.sub(r'\\!([a-zA-Z])', r'. \\1', temp)\n",
    "    temp = temp.split('\\n') #TURNS BACK TO LIST\n",
    "\n",
    "    #deletes speaker name:\n",
    "    arr_speaker_name = []\n",
    "    for i in range(0, len(temp)):\n",
    "        a = temp[i].split()[0:5]  # gets the first 5 words\n",
    "        for j in range(0, len(a)):\n",
    "            if ':' in a[j]:\n",
    "                k = list(a[j])\n",
    "                del k[-1]\n",
    "                p = (a[0:j])\n",
    "                for l in p:\n",
    "                    if l != '' and l not in arr_speaker_name:\n",
    "                        arr_speaker_name.append(l)\n",
    "                if ''.join(k) != '' and ''.join(k) not in arr_speaker_name:\n",
    "                    arr_speaker_name.append(''.join(k))\n",
    "    temp = '\\n'.join(temp)\n",
    "\n",
    "    temp = re.sub(':', ' ', temp)\n",
    "    for i in arr_speaker_name:  # removes all speaker names from transcript\n",
    "        try:\n",
    "            temp = re.sub('\\s+', ' ', temp)  # replace multiple space to single space\n",
    "            temp = re.sub(r'\\s'+i+r'\\s', ' ', temp) #makes sure embedded words aren't deleted, such as 'tim' in estimate\n",
    "            temp = re.sub(r'\\s' + i+r'\\.', ' ', temp)\n",
    "            temp = re.sub(r'\\s' + i+r'\\?', ' ', temp)\n",
    "            temp = re.sub(r'\\s' + i+r'\\,', ' ', temp)\n",
    "            temp = re.sub(r'\\s' + i+r'\\'', ' ', temp)\n",
    "        except:\n",
    "            temp = re.sub('\\s+', ' ', temp)\n",
    "            continue\n",
    "    temp = re.sub('\\s+', ' ', temp)  # replace multiple space to single space\n",
    "    return temp #returns a string\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    #lemmatize and remove stop words and punctuation\n",
    "    stop_words = spacy.load('en_core_web_sm')\n",
    "    #add new stop words\n",
    "    stop_words.Defaults.stop_words.add(\"operator\")\n",
    "    stop_words.Defaults.stop_words.add(\"analyst\")\n",
    "    stop_words.Defaults.stop_words.add(\"quarter\")\n",
    "    stop_words.Defaults.stop_words.add(\"year\")\n",
    "    doc = stop_words(text)\n",
    "    lemmatised_text = \"\"\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            lemma = token.lemma_\n",
    "            if lemma == \"-PRON-\":\n",
    "                lemma = \"it\"\n",
    "            lemmatised_text += (lemma + \" \")\n",
    "    text = lemmatised_text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd716d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ticker = 'UAL'\n",
    "# arr_output = ticker_transcript(ticker)\n",
    "\n",
    "#BRK.B GOOGL? SCHW CEG\n",
    "\n",
    "#AEP ADI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfa3897a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503\n",
      "['STX', 'SEE', 'SRE', 'NOW', 'SHW', 'SBNY', 'SPG', 'SWKS', 'SJM', 'SNA', 'SEDG', 'SO', 'LUV', 'SWK', 'SBUX', 'STT', 'STE', 'SYK', 'SIVB', 'SYF', 'SNPS', 'SYY', 'TMUS', 'TROW', 'TTWO', 'TPR', 'TGT', 'TEL', 'TDY', 'TFX', 'TER', 'TSLA', 'TXN', 'TXT', 'TMO', 'TJX', 'TSCO', 'TT', 'TDG', 'TRV', 'TRMB', 'TFC', 'TWTR', 'TYL', 'TSN', 'USB', 'UDR', 'ULTA', 'UNP', 'UAL', 'UPS', 'URI', 'UNH', 'UHS', 'VLO', 'VTR', 'VRSN', 'VRSK', 'VZ', 'VRTX', 'VFC', 'VTRS', 'VICI', 'V', 'VNO', 'VMC', 'WAB', 'WBA', 'WMT', 'WBD', 'WM', 'WAT', 'WEC', 'WFC', 'WELL', 'WST', 'WDC', 'WRK', 'WY', 'WHR', 'WMB', 'WTW', 'GWW', 'WYNN', 'XEL', 'XYL', 'YUM', 'ZBRA', 'ZBH', 'ZION', 'ZTS']\n"
     ]
    }
   ],
   "source": [
    "print(len(spy_tickers_list))\n",
    "batch = spy_tickers_list[412:]\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "249bd9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STX\n",
      "end\n",
      "SEE\n",
      "end\n",
      "SRE\n",
      "end\n",
      "NOW\n",
      "end\n",
      "SHW\n",
      "end\n",
      "SBNY\n",
      "end\n",
      "SPG\n",
      "end\n",
      "SWKS\n",
      "end\n",
      "SJM\n",
      "end\n",
      "SNA\n",
      "end\n",
      "SEDG\n",
      "end\n",
      "SO\n",
      "end\n",
      "LUV\n",
      "end\n",
      "SWK\n",
      "end\n",
      "SBUX\n",
      "end\n",
      "STT\n",
      "end\n",
      "STE\n",
      "end\n",
      "SYK\n",
      "end\n",
      "SIVB\n",
      "end\n",
      "SYF\n"
     ]
    }
   ],
   "source": [
    "for ticker in batch:\n",
    "    print(ticker)\n",
    "    ticker_output = ticker_transcript(ticker)\n",
    "    df = pd.DataFrame(ticker_output)\n",
    "    arr_na_cleaned = df.dropna()\n",
    "    arr_na_cleaned = arr_na_cleaned.to_numpy()\n",
    "    arr_na_cleaned = arr_na_cleaned[:-1,:]\n",
    "    #delete last row of each ticker where TFIDF = 1\n",
    "    with open('batch2.csv', 'a+') as csvfile:\n",
    "        np.savetxt(csvfile, arr_na_cleaned, delimiter=',', fmt='%s')\n",
    "        \n",
    "    print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655ef167",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = arr_output\n",
    "df = pd.DataFrame(temp)\n",
    "arr_na_cleaned = df.dropna()\n",
    "arr_na_cleaned = arr_na_cleaned.to_numpy()\n",
    "#['ticker', 'yr', 'qtr', '%change', 'tf-idf_cos_sim', LM{'posA' 'negA' 'polA' 'subA' 'posB' 'negB' 'polB' 'subB'}, Complexity{FKsafe harbour' 'FKQ&A' 'GFsafe harbour' 'GFQ&A' ...}] \n",
    "\n",
    "\n",
    "X_data = arr_na_cleaned[:,4:]\n",
    "Y_data = arr_na_cleaned[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2295b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "file = open('batch1.csv', 'rb')\n",
    "data = np.loadtxt(file, delimiter=\",\",dtype='str')\n",
    "\n",
    "print(data)\n",
    "\n",
    "np.savetxt('batch2.csv', data, delimiter=',', fmt='%s')\n",
    "\n",
    "\n",
    "                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e7f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "#scales the X datasets within a range of 0 to 1\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X) \n",
    "\n",
    "#splits the dataset into 70% training and 30% test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a661b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "clf = LinearRegression()\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8064512",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f427616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4461f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e59b782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76180179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('NLP_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6271ad5a9cfee5fbc27e23facc018ff52eb81071ca31a423a1af489ce9841234"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
